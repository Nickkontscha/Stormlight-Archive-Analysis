AllBooks <- AllBooks[AllBooks$book == "Oathbringer",]
AllBooks_tokens <- AllBooks %>%
unnest_tokens(word, text) %>%
filter(!word %in% stopwords) %>%
mutate(word = SnowballC::wordStem(word, language = "english")) %>%
filter(!word %in% c("hand"))
word_counts <- AllBooks_tokens %>%
count(index, word, sort = TRUE) %>%
ungroup() %>%
filter(n > 3)
dtm <- word_counts %>%
cast_dtm(index, word, n)
sel_idx <- slam::row_sums(dtm) > 0
sel_idx <- sort(as.integer(rownames(as.data.frame(sel_idx))))
dtm <- dtm[sel_idx, ]
AllBooks <- AllBooks[sel_idx,]
K <- 6 # number of topics
lda <- LDA(dtm, k = K, control = list(seed = 10))
tidy_lda <- tidy(lda)
top_terms <- tidy_lda %>%
group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>%
ggplot(aes(term, beta, fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 10 terms in each LDA topic",
x = NULL, y = expression(beta)) +
facet_wrap(~ topic, ncol = 4, scales = "free") +
theme_minimal()
##########
library("reshape2")
require(pals)
lda_result <- posterior(lda)
theta <- lda_result$topics
beta <- lda_result$terms
topicNames <- apply(lda::top.topic.words(beta, 7, by.score = T), 2, paste, collapse = " ")
topic_proportion <- aggregate(theta, by = list(chapter = AllBooks$index), mean)
# set topic names to aggregated columns
colnames(topic_proportion)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion, id.vars = "chapter")
ggplot(vizDataFrame, aes(x=chapter, y=value, fill=variable)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "chapter") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
# set topic names to aggregated columns
colnames(topic_proportion)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion, id.vars = "chapter")
topic_proportion
AllBooks$index
source("loadData/loadData.R")
AllBooks <- AllBooks[, c("chapter", "nr", "text", "character", "book", "flashback")]
AllBooks <- AllBooks[!is.na(AllBooks$nr),]
AllBooks <- AllBooks[!duplicated(AllBooks$chapter), ]
AllBooks$index <- seq(1, nrow(AllBooks), 1)
rm(list=setdiff(ls(), "AllBooks"))
stopwords <- unique(stop_words$word)
stopwords <- c(stopwords, gsub("[^[:alnum:][:space:]]", "", stopwords))
stopwords <- unique(stopwords)
AllBooks <- AllBooks[AllBooks$book == "Oathbringer",]
AllBooks_tokens <- AllBooks %>%
unnest_tokens(word, text) %>%
filter(!word %in% stopwords) %>%
mutate(word = SnowballC::wordStem(word, language = "english")) %>%
filter(!word %in% c("hand"))
word_counts <- AllBooks_tokens %>%
count(index, word, sort = TRUE) %>%
ungroup() %>%
filter(n > 3)
dtm <- word_counts %>%
cast_dtm(index, word, n)
sel_idx <- slam::row_sums(dtm) > 0
sel_idx <- sort(as.integer(rownames(as.data.frame(sel_idx))))
dtm <- dtm[sel_idx, ]
AllBooks <- AllBooks[sel_idx,]
K <- 6 # number of topics
lda <- LDA(dtm, k = K, control = list(seed = 10))
tidy_lda <- tidy(lda)
top_terms <- tidy_lda %>%
group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>%
ggplot(aes(term, beta, fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 10 terms in each LDA topic",
x = NULL, y = expression(beta)) +
facet_wrap(~ topic, ncol = 4, scales = "free") +
theme_minimal()
##########
library("reshape2")
require(pals)
lda_result <- posterior(lda)
theta <- lda_result$topics
beta <- lda_result$terms
topicNames <- apply(lda::top.topic.words(beta, 7, by.score = T), 2, paste, collapse = " ")
topic_proportion <- aggregate(theta, by = list(chapter = AllBooks$index), mean)
# set topic names to aggregated columns
colnames(topic_proportion)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion, id.vars = "chapter")
topic_proportion
AllBooks$index
names(AllBooks)
AllBooks$nr
source("loadData/loadData.R")
AllBooks <- AllBooks[, c("chapter", "nr", "text", "character", "book", "flashback")]
AllBooks <- AllBooks[!is.na(AllBooks$nr),]
AllBooks <- AllBooks[!duplicated(AllBooks$chapter), ]
AllBooks$index <- seq(1, nrow(AllBooks), 1)
rm(list=setdiff(ls(), "AllBooks"))
View(AllBooks)
stopwords <- unique(stop_words$word)
stopwords <- c(stopwords, gsub("[^[:alnum:][:space:]]", "", stopwords))
stopwords <- unique(stopwords)
AllBooks <- AllBooks[AllBooks$book == "Oathbringer",]
AllBooks_tokens <- AllBooks %>%
unnest_tokens(word, text) %>%
filter(!word %in% stopwords) %>%
mutate(word = SnowballC::wordStem(word, language = "english")) %>%
filter(!word %in% c("hand"))
word_counts <- AllBooks_tokens %>%
count(index, word, sort = TRUE) %>%
ungroup() %>%
filter(n > 3)
unique(AllBooks$index)
AllBooks_tokens <- AllBooks %>%
unnest_tokens(word, text) %>%
filter(!word %in% stopwords) %>%
mutate(word = SnowballC::wordStem(word, language = "english")) %>%
filter(!word %in% c("hand"))
word_counts <- AllBooks_tokens %>%
count(index, word, sort = TRUE) %>%
ungroup() %>%
filter(n > 3)
dtm <- word_counts %>%
cast_dtm(index, word, n)
sel_idx <- slam::row_sums(dtm) > 0
sel_idx <- sort(as.integer(rownames(as.data.frame(sel_idx))))
dtm <- dtm[sel_idx, ]
sel_idx
unique(AllBooks$index)
AllBooks$index <- seq(1, nrow(AllBooks), 1)
AllBooks_tokens <- AllBooks %>%
unnest_tokens(word, text) %>%
filter(!word %in% stopwords) %>%
mutate(word = SnowballC::wordStem(word, language = "english")) %>%
filter(!word %in% c("hand"))
word_counts <- AllBooks_tokens %>%
count(index, word, sort = TRUE) %>%
ungroup() %>%
filter(n > 3)
dtm <- word_counts %>%
cast_dtm(index, word, n)
sel_idx <- slam::row_sums(dtm) > 0
sel_idx <- sort(as.integer(rownames(as.data.frame(sel_idx))))
dtm <- dtm[sel_idx, ]
AllBooks <- AllBooks[sel_idx,]
K <- 6 # number of topics
lda <- LDA(dtm, k = K, control = list(seed = 10))
tidy_lda <- tidy(lda)
top_terms <- tidy_lda %>%
group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>%
ggplot(aes(term, beta, fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 10 terms in each LDA topic",
x = NULL, y = expression(beta)) +
facet_wrap(~ topic, ncol = 4, scales = "free") +
theme_minimal()
##########
library("reshape2")
require(pals)
lda_result <- posterior(lda)
theta <- lda_result$topics
beta <- lda_result$terms
topicNames <- apply(lda::top.topic.words(beta, 7, by.score = T), 2, paste, collapse = " ")
topic_proportion <- aggregate(theta, by = list(chapter = AllBooks$index), mean)
# set topic names to aggregated columns
colnames(topic_proportion)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion, id.vars = "chapter")
ggplot(vizDataFrame, aes(x=chapter, y=value, fill=variable)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "chapter") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
load("C:/Users/swesd/Desktop/NLP/SA_nlp/correlation.RData")
dend %>% set("branches_k_color", k = clusterNr) %>% set("labels_col", k= clusterNr) %>% plot(horiz = TRUE)
library(rvest)
library(textfeatures)
library(rtweet)
library(tidyverse)
library(corrplot)
library(maptree) # dendogram
library(dendextend)
par(mar = c(0,0,0,10))
dend %>% set("branches_k_color", k = clusterNr) %>% set("labels_col", k= clusterNr) %>% plot(horiz = TRUE)
dend %>% rect.dendrogram(k = clusterNr, horiz = TRUE, border = 8, lty = 5, lwd = 1)
# https://github.com/mkearney/resist_oped/blob/master/R/01-features.R
#library(devtools)
#assignInNamespace("version_info", c(devtools:::version_info, list("3.5" = list(version_min = "3.3.0", version_max = "99.99.99", path = "bin"))), "devtools")
#devtools::install_github("mkearney/textfeatures")
library(rvest)
library(textfeatures)
library(rtweet)
library(tidyverse)
library(corrplot)
library(maptree) # dendogram
library(dendextend)
source("loadData/loadData.R")
AllBooks <- AllBooks[, c("chapter", "nr", "text", "character", "book", "flashback")]
AllBooks <- AllBooks[!is.na(AllBooks$nr),]
rm(list=setdiff(ls(), "AllBooks"))
# Create data set with just an id and text as required by textfeatures
data <- data_frame(
id = AllBooks$character,
text = AllBooks$text
)
# Feature extraction
# Computation intensive: I set it to numberOfLogicalCores - 1
tf <- textfeatures::textfeatures(data, word_dims = 150, threads = 3)
# Summarise by id
tfsum <- tf %>%
group_by(id) %>%
summarise_all(mean, na.rm = TRUE) %>%
ungroup()
# Vector of unique characters
characters <- unique(tfsum$id)
# Create numeric vectors of equal length for each id
cols <- map(characters,
~ filter(tfsum, id == .x) %>% select(-id) %>% as.list() %>% unlist()
)
# Create matrix
mat <- cols %>%
unlist() %>%
as.numeric() %>%
matrix(nrow = length(characters), byrow = TRUE)
# Set row and column names
row.names(mat) <- characters
# Plot correlations
cor <- cor(t(mat))
#cor <- abs(cor)
corrplot(cor, order = "hclust", addrect = 6)
dissimilarity <- 1 - cor
#dissimilarity <- 1 - abs(cor)
distance <- as.dist(dissimilarity)
#plot(hclust(distance), main="Dissimilarity = 1 - |Correlation|", xlab="")
# Find best clusters for dendogramm
dend <- hclust(distance)
op_k <- kgs(dend, distance, maxclus = 20)
# plot (names (op_k), op_k, xlab="# clusters", ylab="penalty")
op_k[which(op_k == min(op_k))]
clusterNr <- as.integer(names(op_k[which(op_k == min(op_k))]))
dend <- distance %>% hclust() %>% as.dendrogram
par(mar = c(0,0,0,10))
dend %>% set("branches_k_color", k = clusterNr) %>% set("labels_col", k= clusterNr) %>% plot(horiz = TRUE)
dend %>% rect.dendrogram(k = clusterNr, horiz = TRUE, border = 8, lty = 5, lwd = 1)
# Character information from:
# Those are nice lists prepared by the community which gives us some information about who a chapter is about
# https://coppermind.net/wiki/The_Way_of_Kings/Statistical_analysis
# https://coppermind.net/wiki/Words_of_Radiance/Statistical_analysis
# https://coppermind.net/wiki/Oathbringer/Statistical_analysis
# A function to read the given excel file from a path with a few string operations to get them in a better format
getCharacters <- function(path){
files <- list.files(path)
files <- paste(path, files, sep = "")
chars <- data.frame(matrix(ncol = 4, nrow = 0))
for(i in 1:length(files)){
chars <- rbind(chars, read.xlsx(files[i], startRow = 2, colNames = F))
}
names(chars) <- c("chapter", "character", "wordCount", "percentage")
for(i in 1:nrow(chars)){
if(is.na(chars$chapter[i])){
chars$chapter[i] <- chars$chapter[i - 1]
}
}
chars$wordCount <- NULL
chars$percentage <- NULL
chars$chapter <- gsub(".*: ","",chars$chapter)
chars$chapter <- gsub(".*\\. ","",chars$chapter)
chars$chapter <- gsub("[[:punct:]]", "", chars$chapter)
chars$chapter <- gsub("[^[:alnum:][:space:]]", "", chars$chapter)
chars$chapter <- gsub("^\\s+|\\s+$", "", chars$chapter)
chars$chapter <- tolower(chars$chapter)
#chars$character <- tolower(chars$character)
chars$flashback <- FALSE
for(i in 1:nrow(chars)){
if(grepl("(flashback)", chars$character[i])){
chars$character[i] <- gsub("\\s\\(flashback\\)", "", chars$character[i])
chars$flashback[i] <- TRUE
}
}
for(i in 1:nrow(chars)){
if(chars$character[i] == "Szeth"){
chars$character[i] <- "Szeth-son-son-Vallano"
}
}
return(chars)
}
# Get the characters of each book
char_wok <- getCharacters("data/char/way_of_kings/")
char_wor <- getCharacters("data/char/words_of_radiance/")
char_oath <- getCharacters("data/char/oathbringer/")
char_wok <- unique(char_wok)
char_wor <- unique(char_wor)
char_oath <- unique(char_oath)
# I encountered a few errors on either the excel files or the names of the chapter in my .epub versions.
# So I have to fix them here
char_wok$chapter[which(char_wok$chapter == "eyes hands or spears")] <- "eyes hands or spheres"
char_oath$chapter[which(char_oath$chapter == "the others may stand")] <- "that others may stand"
char_oath$chapter[which(char_oath$chapter == "set up to fall")] <- "set up to fail"
char_oath$chapter[which(char_oath$chapter == "elista")] <- "ellista"
char_oath$chapter[which(char_oath$chapter == "bright side")] <- "the bright side"
# Get all characters
all_chars <- c(char_wok$character,
char_wor$character,
char_oath$character)
# and their frequencies
all_chars <- as.data.frame(table(all_chars))
rm(getCharacters)
all_chars
View(all_chars)
head(all_chars)
head(order(all_chars))
head(all_chars[order(all_chars)])
head(all_chars[order(all_chars$Freq)])
head(all_chars[order(all_chars$Freq),])
?order
head(all_chars[order(all_chars$Freq, decreasing = T),])
library(rvest)
library(textfeatures)
library(rtweet)
library(tidyverse)
library(corrplot)
library(maptree) # dendogram
library(dendextend)
source("loadData/loadData.R")
AllBooks <- AllBooks[, c("chapter", "nr", "text", "character", "book", "flashback")]
AllBooks <- AllBooks[!is.na(AllBooks$nr),]
rm(list=setdiff(ls(), "AllBooks"))
AllBooks$character
table(AllBooks$character)
head(table(AllBooks$character)[order(table(AllBooks$character)$Freq, decreasing = T),])
head(table(AllBooks$character)[order(table(AllBooks$character)$Freq, decreasing = T)])
head(table(AllBooks$character)[order(table(AllBooks$character), decreasing = T)])
head(table(AllBooks$character)[order(table(AllBooks$character), decreasing = T)])
all_chars <- as.data.frame(table(all_chars))
all_chars <- as.data.frame(table(AllBooks$character))
head(all_chars[order(all_chars$freq, decreasing = T),])
all_chars$Freq
head(all_chars[order(all_chars$Freq, decreasing = T),])
head(all_chars[order(all_chars$Freq, decreasing = T),], 7)
head(all_chars[order(all_chars$Freq, decreasing = T),], 10)
knitr::opts_chunk$set(echo = TRUE)
all_chars <- as.data.frame(table(AllBooks$character))
names(all_chars) <- c("Name", "Frequency")
head(all_chars[order(all_chars$Freq, decreasing = T),])
all_chars <- as.data.frame(table(AllBooks$character))
names(all_chars) <- c("Name", "Frequency")
rownames(all_chars) <- NA
all_chars <- as.data.frame(table(AllBooks$character))
names(all_chars) <- c("Name", "Frequency")
rownames(all_chars) <- NULL
head(all_chars[order(all_chars$Freq, decreasing = T),])
all_chars
rownames(all_chars)
rownames(all_chars) <- NULL
all_chars
all_chars <- as.data.frame(table(AllBooks$character))
names(all_chars) <- c("Name", "Frequency")
rownames(all_chars) <- c()
head(all_chars[order(all_chars$Freq, decreasing = T),])
all_chars <- as.data.frame(table(AllBooks$character))
names(all_chars) <- c("Name", "Frequency")
all_chars <- all_chars[order(all_chars$Frequency, decreasing = T),]
head(all_chars)
all_chars <- as.data.frame(table(AllBooks$character))
names(all_chars) <- c("Name", "Frequency")
all_chars <- all_chars[order(all_chars$Frequency, decreasing = T),]
rownames(all_chars) <- c()
head(all_chars)
all_chars <- as.data.frame(table(AllBooks$character))
names(all_chars) <- c("Name", "Frequency")
all_chars <- all_chars[order(all_chars$Frequency, decreasing = T),]
rownames(all_chars) <- NULL
head(all_chars)
stopwords <- unique(stop_words$word)
stopwords <- c(stopwords, gsub("[^[:alnum:][:space:]]", "", stopwords))
stopwords <- unique(stopwords)
stopwords_regex = paste(stopwords, collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
AllBooks$text = str_replace_all(AllBooks$text, stopwords_regex, '')
# Create data set with just an id and text as required by textfeatures
data <- data_frame(
id = AllBooks$character,
text = AllBooks$text
)
# Feature extraction
# Computation intensive: I set it to numberOfLogicalCores - 1
tf <- textfeatures::textfeatures(data, word_dims = 150, threads = 3)
# Summarise by id
tfsum <- tf %>%
group_by(id) %>%
summarise_all(mean, na.rm = TRUE) %>%
ungroup()
# Vector of unique characters
characters <- unique(tfsum$id)
# Create numeric vectors of equal length for each id
cols <- map(characters,
~ filter(tfsum, id == .x) %>% select(-id) %>% as.list() %>% unlist()
)
# Create matrix
mat <- cols %>%
unlist() %>%
as.numeric() %>%
matrix(nrow = length(characters), byrow = TRUE)
# Set row and column names
row.names(mat) <- characters
# Plot correlations
cor <- cor(t(mat))
#cor <- abs(cor)
corrplot(cor, order = "hclust", addrect = 6)
dissimilarity <- 1 - cor
#cor <- abs(cor)
corrplot(cor, order = "hclust", addrect = 7)
#cor <- abs(cor)
corrplot(cor, order = "hclust", addrect = 6)
dissimilarity <- 1 - cor
#dissimilarity <- 1 - abs(cor)
distance <- as.dist(dissimilarity)
# Find best clusters for dendogramm
dend <- hclust(distance)
op_k <- kgs(dend, distance, maxclus = 20)
# plot (names (op_k), op_k, xlab="# clusters", ylab="penalty")
op_k[which(op_k == min(op_k))]
clusterNr <- as.integer(names(op_k[which(op_k == min(op_k))]))
dend <- distance %>% hclust() %>% as.dendrogram
par(mar = c(0,0,0,10))
dend %>% set("branches_k_color", k = clusterNr) %>% set("labels_col", k= clusterNr) %>% plot(horiz = TRUE)
dend %>% rect.dendrogram(k = clusterNr, horiz = TRUE, border = 8, lty = 5, lwd = 1)
# Feature extraction
# Computation intensive: I set it to numberOfLogicalCores - 1
tf <- textfeatures::textfeatures(data, word_dims = 300, threads = 3)
# Summarise by id
tfsum <- tf %>%
group_by(id) %>%
summarise_all(mean, na.rm = TRUE) %>%
ungroup()
# Vector of unique characters
characters <- unique(tfsum$id)
# Create numeric vectors of equal length for each id
cols <- map(characters,
~ filter(tfsum, id == .x) %>% select(-id) %>% as.list() %>% unlist()
)
# Create matrix
mat <- cols %>%
unlist() %>%
as.numeric() %>%
matrix(nrow = length(characters), byrow = TRUE)
# Set row and column names
row.names(mat) <- characters
# Plot correlations
cor <- cor(t(mat))
#cor <- abs(cor)
corrplot(cor, order = "hclust", addrect = 6)
dissimilarity <- 1 - cor
#dissimilarity <- 1 - abs(cor)
distance <- as.dist(dissimilarity)
#plot(hclust(distance), main="Dissimilarity = 1 - |Correlation|", xlab="")
# Find best clusters for dendogramm
dend <- hclust(distance)
op_k <- kgs(dend, distance, maxclus = 20)
# plot (names (op_k), op_k, xlab="# clusters", ylab="penalty")
op_k[which(op_k == min(op_k))]
clusterNr <- as.integer(names(op_k[which(op_k == min(op_k))]))
dend <- distance %>% hclust() %>% as.dendrogram
par(mar = c(0,0,0,10))
dend %>% set("branches_k_color", k = clusterNr) %>% set("labels_col", k= clusterNr) %>% plot(horiz = TRUE)
dend %>% rect.dendrogram(k = clusterNr, horiz = TRUE, border = 8, lty = 5, lwd = 1)
#cor <- abs(cor)
corrplot(cor, order = "hclust", addrect = 6)
